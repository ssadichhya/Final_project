{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, FloatType, IntegerType, DateType\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"final_project\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .config(\"spark.jars\", \"/usr/lib/jvm/java-11-openjdk-amd64/lib/postgresql-42.5.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "        \n",
    "    #csv path\n",
    "    csv = \"/home/ubuntu/Desktop/final_project/Raw_Data/job_descriptions.csv\"\n",
    "    #Read raw_data\n",
    "    df = spark.read.csv(csv, header=True, inferSchema=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf to calculate avg\n",
    "def calculate_average(range_str):\n",
    "    # Use regular expression to extract numbers\n",
    "    numbers = re.findall(r'\\d+', range_str)\n",
    "    if len(numbers) == 2:\n",
    "        lower = int(numbers[0])\n",
    "        upper = int(numbers[1])\n",
    "        avg = (lower + upper) / 2\n",
    "        return avg\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform():\n",
    "    df=extract()\n",
    "    #List of columns to drop \n",
    "    dropped = [\"Contact Person\", \"Contact\", \"Benefits\",\"Company Profile\"]\n",
    "    #Drop column\n",
    "    df = df.drop(*dropped)\n",
    "    #changing to standard datatype\n",
    "    df = df.withColumn(\"Job Id\", df[\"Job Id\"].cast(LongType()))\\\n",
    "        .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType()))\\\n",
    "         .withColumn(\"longitude\", df[\"longitude\"].cast(FloatType()))\\\n",
    "          .withColumn(\"Company Size\", df[\"Company Size\"].cast(IntegerType()))\\\n",
    "           .withColumn(\"Job Posting Date\", df[\"Job Posting Date\"].cast(DateType()))\n",
    "    calculate_average_udf = F.udf(calculate_average)\n",
    "\n",
    "    # Add a new column with the calculated average\n",
    "    new_df = df.withColumn(\"Average\", calculate_average_udf(df[\"Salary Range\"]))\n",
    "    new_df.withColumnRenamed(\"Average\",\"Average Salary\").printSchema()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    new_df=transform()\n",
    "    ##Load the clean data in postgres\n",
    "    new_df.write.format('jdbc').options(url='jdbc:postgresql://localhost:5432/final_project',driver = 'org.postgresql.Driver', dbtable = 'job_description_clean', user=\"postgres\",password=\"postgres\" ).mode('overwrite').save()\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 16:08:20 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
      "23/10/27 16:08:20 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "23/10/27 16:08:20 INFO PythonUDFRunner: Times: total = 2668, boot = 77, init = 187, finish = 2404\n",
      "23/10/27 16:08:20 INFO PythonUDFRunner: Times: total = 3141, boot = 210, init = 302, finish = 2629\n",
      "23/10/27 16:08:20 INFO PythonUDFRunner: Times: total = 3006, boot = 105, init = 133, finish = 2768\n",
      "23/10/27 16:08:20 INFO Executor: Finished task 7.0 in stage 16.0 (TID 80). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:20 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 80) in 16596 ms on 172.16.5.112 (executor driver) (1/16)\n",
      "23/10/27 16:08:20 INFO Executor: Finished task 1.0 in stage 16.0 (TID 74). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:20 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 74) in 16608 ms on 172.16.5.112 (executor driver) (2/16)\n",
      "23/10/27 16:08:20 INFO Executor: Finished task 3.0 in stage 16.0 (TID 76). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:20 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 76) in 16612 ms on 172.16.5.112 (executor driver) (3/16)\n",
      "23/10/27 16:08:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/10/27 16:08:20 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#5594, None)) > 0)\n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.2 KiB, free 177.3 MiB)\n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 177.2 MiB)\n",
      "23/10/27 16:08:21 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.16.5.112:41715 (size: 34.6 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:21 INFO SparkContext: Created broadcast 21 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/10/27 16:08:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 109210027 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/10/27 16:08:21 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Got job 14 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Final stage: ResultStage 17 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[112] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 12.1 KiB, free 177.2 MiB)\n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 177.2 MiB)\n",
      "23/10/27 16:08:21 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.16.5.112:41715 (size: 6.0 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:21 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[112] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/27 16:08:21 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 89) (172.16.5.112, executor driver, partition 0, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:21 INFO Executor: Running task 0.0 in stage 17.0 (TID 89)\n",
      "23/10/27 16:08:21 INFO FileScanRDD: Reading File path: file:///home/ubuntu/Desktop/final_project/Raw_Data/job_descriptions.csv, range: 0-109210027, partition values: [empty row]\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 0.0 in stage 17.0 (TID 89). 1797 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 89) in 48 ms on 172.16.5.112 (executor driver) (1/1)\n",
      "23/10/27 16:08:21 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "23/10/27 16:08:21 INFO DAGScheduler: ResultStage 17 (csv at NativeMethodAccessorImpl.java:0) finished in 0.148 s\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/27 16:08:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Job 14 finished: csv at NativeMethodAccessorImpl.java:0, took 0.153839 s\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2724, boot = 166, init = 266, finish = 2292\n",
      "23/10/27 16:08:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/10/27 16:08:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 200.2 KiB, free 177.0 MiB)\n",
      "23/10/27 16:08:21 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 177.0 MiB)\n",
      "23/10/27 16:08:21 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.16.5.112:41715 (size: 34.6 KiB, free: 177.4 MiB)\n",
      "23/10/27 16:08:21 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.16.5.112:41715 in memory (size: 6.0 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:21 INFO SparkContext: Created broadcast 23 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 5.0 in stage 16.0 (TID 78). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 109210027 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 78) in 16972 ms on 172.16.5.112 (executor driver) (4/16)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 3121, boot = 119, init = 268, finish = 2734\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 3079, boot = 319, init = 265, finish = 2495\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2997, boot = 11, init = 93, finish = 2893\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2378, boot = 36, init = 115, finish = 2227\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 13.0 in stage 16.0 (TID 86). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 0.0 in stage 16.0 (TID 73). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 13.0 in stage 16.0 (TID 86) in 17068 ms on 172.16.5.112 (executor driver) (5/16)\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 73) in 17074 ms on 172.16.5.112 (executor driver) (6/16)\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 10.0 in stage 16.0 (TID 83). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 83) in 17077 ms on 172.16.5.112 (executor driver) (7/16)\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 2.0 in stage 16.0 (TID 75). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 75) in 17120 ms on 172.16.5.112 (executor driver) (8/16)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2699, boot = 59, init = 207, finish = 2433\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 4.0 in stage 16.0 (TID 77). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 77) in 17178 ms on 172.16.5.112 (executor driver) (9/16)\n",
      "23/10/27 16:08:21 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.16.5.112:41715 in memory (size: 34.6 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2996, boot = 241, init = 387, finish = 2368\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 11.0 in stage 16.0 (TID 84). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 84) in 17270 ms on 172.16.5.112 (executor driver) (10/16)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2956, boot = 66, init = 249, finish = 2641\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2872, boot = 465, init = 251, finish = 2156\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 3215, boot = 436, init = 282, finish = 2497\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 8.0 in stage 16.0 (TID 81). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 81) in 17348 ms on 172.16.5.112 (executor driver) (11/16)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 3045, boot = 174, init = 357, finish = 2514\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 9.0 in stage 16.0 (TID 82). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 15.0 in stage 16.0 (TID 88). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 15.0 in stage 16.0 (TID 88) in 17376 ms on 172.16.5.112 (executor driver) (12/16)\n",
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2973, boot = 17, init = 136, finish = 2820\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 82) in 17378 ms on 172.16.5.112 (executor driver) (13/16)\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 14.0 in stage 16.0 (TID 87). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 14.0 in stage 16.0 (TID 87) in 17411 ms on 172.16.5.112 (executor driver) (14/16)\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 12.0 in stage 16.0 (TID 85). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 12.0 in stage 16.0 (TID 85) in 17428 ms on 172.16.5.112 (executor driver) (15/16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job Id: long (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Qualifications: string (nullable = true)\n",
      " |-- Salary Range: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- Work Type: string (nullable = true)\n",
      " |-- Company Size: integer (nullable = true)\n",
      " |-- Job Posting Date: date (nullable = true)\n",
      " |-- Preference: string (nullable = true)\n",
      " |-- Job Title: string (nullable = true)\n",
      " |-- Role: string (nullable = true)\n",
      " |-- Job Portal: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- skills: string (nullable = true)\n",
      " |-- Responsibilities: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Average Salary: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 16:08:21 INFO PythonUDFRunner: Times: total = 2565, boot = 12, init = 153, finish = 2400\n",
      "23/10/27 16:08:21 INFO Executor: Finished task 6.0 in stage 16.0 (TID 79). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:21 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 79) in 17540 ms on 172.16.5.112 (executor driver) (16/16)\n",
      "23/10/27 16:08:21 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "23/10/27 16:08:21 INFO DAGScheduler: ResultStage 16 (save at NativeMethodAccessorImpl.java:0) finished in 17.595 s\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/27 16:08:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "23/10/27 16:08:21 INFO DAGScheduler: Job 13 finished: save at NativeMethodAccessorImpl.java:0, took 17.600803 s\n",
      "23/10/27 16:08:22 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Got job 15 (save at NativeMethodAccessorImpl.java:0) with 16 output partitions\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Final stage: ResultStage 18 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[127] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/27 16:08:22 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 57.1 KiB, free 177.2 MiB)\n",
      "23/10/27 16:08:22 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 177.2 MiB)\n",
      "23/10/27 16:08:22 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.16.5.112:41715 (size: 23.5 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:22 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535\n",
      "23/10/27 16:08:22 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 18 (MapPartitionsRDD[127] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "23/10/27 16:08:22 INFO TaskSchedulerImpl: Adding task set 18.0 with 16 tasks resource profile 0\n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 90) (172.16.5.112, executor driver, partition 0, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 91) (172.16.5.112, executor driver, partition 1, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 92) (172.16.5.112, executor driver, partition 2, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 93) (172.16.5.112, executor driver, partition 3, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 4.0 in stage 18.0 (TID 94) (172.16.5.112, executor driver, partition 4, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 5.0 in stage 18.0 (TID 95) (172.16.5.112, executor driver, partition 5, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 6.0 in stage 18.0 (TID 96) (172.16.5.112, executor driver, partition 6, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 7.0 in stage 18.0 (TID 97) (172.16.5.112, executor driver, partition 7, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 8.0 in stage 18.0 (TID 98) (172.16.5.112, executor driver, partition 8, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 9.0 in stage 18.0 (TID 99) (172.16.5.112, executor driver, partition 9, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 10.0 in stage 18.0 (TID 100) (172.16.5.112, executor driver, partition 10, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 11.0 in stage 18.0 (TID 101) (172.16.5.112, executor driver, partition 11, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 12.0 in stage 18.0 (TID 102) (172.16.5.112, executor driver, partition 12, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 13.0 in stage 18.0 (TID 103) (172.16.5.112, executor driver, partition 13, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 14.0 in stage 18.0 (TID 104) (172.16.5.112, executor driver, partition 14, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO TaskSetManager: Starting task 15.0 in stage 18.0 (TID 105) (172.16.5.112, executor driver, partition 15, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:22 INFO Executor: Running task 6.0 in stage 18.0 (TID 96)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 0.0 in stage 18.0 (TID 90)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 1.0 in stage 18.0 (TID 91)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 2.0 in stage 18.0 (TID 92)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 9.0 in stage 18.0 (TID 99)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 11.0 in stage 18.0 (TID 101)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 12.0 in stage 18.0 (TID 102)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 14.0 in stage 18.0 (TID 104)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 15.0 in stage 18.0 (TID 105)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 3.0 in stage 18.0 (TID 93)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 4.0 in stage 18.0 (TID 94)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 7.0 in stage 18.0 (TID 97)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 8.0 in stage 18.0 (TID 98)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 10.0 in stage 18.0 (TID 100)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 5.0 in stage 18.0 (TID 95)\n",
      "23/10/27 16:08:22 INFO Executor: Running task 13.0 in stage 18.0 (TID 103)\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_14 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_10 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_0 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_11 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_3 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_8 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_6 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_15 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_12 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_7 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_5 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_9 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_1 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_13 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_2 locally\n",
      "23/10/27 16:08:22 INFO BlockManager: Found block rdd_13_4 locally\n",
      "23/10/27 16:08:22 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.16.5.112:41715 in memory (size: 23.5 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 93\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@62dac72a,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 13.6 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 93 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 107556259 bytes of memory are used for execution and 269659557 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 100\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@42f33275,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 13.6 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 100 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 145907558 bytes of memory are used for execution and 269659557 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO MemoryStore: 1 blocks selected for dropping (200.2 KiB bytes)\n",
      "23/10/27 16:08:22 INFO BlockManager: Dropping block broadcast_23 from memory\n",
      "23/10/27 16:08:22 INFO BlockManager: Writing block broadcast_23 to disk\n",
      "23/10/27 16:08:22 INFO MemoryStore: After dropping 1 blocks, free memory is 1157.1 KiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 98\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@46a08b07,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 11.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 98 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 99\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@71c0e8e7,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 10.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 97\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 103\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 91\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 92\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@67d94b62,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 10.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@2a664b5f,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 13.6 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@570c9fa7,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 11.1 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@3ed23095,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 11.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 91 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 90\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@755ccadc,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 10.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 90 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 94\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@473ff24d,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 13.6 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 94 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 95\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 101\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 96\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@21484198,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 10.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 104\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 1048560 bytes of memory were used by task 96 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@3e97f226,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 12.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 104 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@647efde4,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 10.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 1048560 bytes of memory were used by task 95 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 186046897 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Memory used in task 105\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@1c48148a,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 8.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 105 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 183949777 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@299378af,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 9.0 MiB\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 101 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 183949777 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 92 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 183949777 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 103 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 97 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 183949777 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 183949777 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 0 bytes of memory were used by task 99 but are not associated with specific consumers\n",
      "23/10/27 16:08:22 INFO TaskMemoryManager: 184998337 bytes of memory are used for execution and 269454517 bytes of memory are used for storage\n",
      "23/10/27 16:08:23 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.16.5.112:41715 in memory (size: 34.6 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:23 INFO TaskMemoryManager: Memory used in task 102\n",
      "23/10/27 16:08:23 INFO TaskMemoryManager: Acquired by HybridRowQueue(org.apache.spark.memory.TaskMemoryManager@44494fd3,/tmp/spark-e8ebaae7-6c2d-4ba7-be73-422f16cbaebc,19,org.apache.spark.serializer.SerializerManager@469496f6): 13.6 MiB\n",
      "23/10/27 16:08:23 INFO TaskMemoryManager: 0 bytes of memory were used by task 102 but are not associated with specific consumers\n",
      "23/10/27 16:08:23 INFO TaskMemoryManager: 184552916 bytes of memory are used for execution and 269419081 bytes of memory are used for storage\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2733, boot = -14605, init = 14933, finish = 2405\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2479, boot = -14528, init = 14834, finish = 2173\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 10.0 in stage 18.0 (TID 100). 2216 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 10.0 in stage 18.0 (TID 100) in 16202 ms on 172.16.5.112 (executor driver) (1/16)\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 15.0 in stage 18.0 (TID 105). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 15.0 in stage 18.0 (TID 105) in 16222 ms on 172.16.5.112 (executor driver) (2/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2515, boot = -15103, init = 15371, finish = 2247\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 4.0 in stage 18.0 (TID 94). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 4.0 in stage 18.0 (TID 94) in 16293 ms on 172.16.5.112 (executor driver) (3/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2841, boot = -14589, init = 14868, finish = 2562\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 12.0 in stage 18.0 (TID 102). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 12.0 in stage 18.0 (TID 102) in 16406 ms on 172.16.5.112 (executor driver) (4/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2785, boot = -14733, init = 15025, finish = 2493\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 13.0 in stage 18.0 (TID 103). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 13.0 in stage 18.0 (TID 103) in 16583 ms on 172.16.5.112 (executor driver) (5/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2790, boot = -15027, init = 15288, finish = 2529\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2677, boot = -15344, init = 15629, finish = 2392\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2716, boot = -14682, init = 14990, finish = 2408\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2904, boot = -14732, init = 14994, finish = 2642\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 11.0 in stage 18.0 (TID 101). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 11.0 in stage 18.0 (TID 101) in 16650 ms on 172.16.5.112 (executor driver) (6/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2738, boot = -14615, init = 14974, finish = 2379\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 14.0 in stage 18.0 (TID 104). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 14.0 in stage 18.0 (TID 104) in 16667 ms on 172.16.5.112 (executor driver) (7/16)\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 7.0 in stage 18.0 (TID 97). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 7.0 in stage 18.0 (TID 97) in 16672 ms on 172.16.5.112 (executor driver) (8/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2311, boot = -14727, init = 14962, finish = 2076\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 2.0 in stage 18.0 (TID 92). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 92) in 16679 ms on 172.16.5.112 (executor driver) (9/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2990, boot = -14759, init = 15037, finish = 2712\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2879, boot = -15003, init = 15280, finish = 2602\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 5.0 in stage 18.0 (TID 95). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 5.0 in stage 18.0 (TID 95) in 16688 ms on 172.16.5.112 (executor driver) (10/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2341, boot = -14708, init = 14985, finish = 2064\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 3.0 in stage 18.0 (TID 93). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 93) in 16702 ms on 172.16.5.112 (executor driver) (11/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2962, boot = -15025, init = 15297, finish = 2690\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 6.0 in stage 18.0 (TID 96). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 8.0 in stage 18.0 (TID 98). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 6.0 in stage 18.0 (TID 96) in 16711 ms on 172.16.5.112 (executor driver) (12/16)\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 8.0 in stage 18.0 (TID 98) in 16713 ms on 172.16.5.112 (executor driver) (13/16)\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 0.0 in stage 18.0 (TID 90). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 90) in 16720 ms on 172.16.5.112 (executor driver) (14/16)\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 1.0 in stage 18.0 (TID 91). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 91) in 16730 ms on 172.16.5.112 (executor driver) (15/16)\n",
      "23/10/27 16:08:38 INFO PythonUDFRunner: Times: total = 2808, boot = -14846, init = 15117, finish = 2537\n",
      "23/10/27 16:08:38 INFO Executor: Finished task 9.0 in stage 18.0 (TID 99). 2173 bytes result sent to driver\n",
      "23/10/27 16:08:38 INFO TaskSetManager: Finished task 9.0 in stage 18.0 (TID 99) in 16789 ms on 172.16.5.112 (executor driver) (16/16)\n",
      "23/10/27 16:08:38 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "23/10/27 16:08:38 INFO DAGScheduler: ResultStage 18 (save at NativeMethodAccessorImpl.java:0) finished in 16.819 s\n",
      "23/10/27 16:08:38 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/27 16:08:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "23/10/27 16:08:38 INFO DAGScheduler: Job 15 finished: save at NativeMethodAccessorImpl.java:0, took 16.824349 s\n",
      "23/10/27 16:08:39 INFO CodeGenerator: Code generated in 16.684438 ms            \n",
      "23/10/27 16:08:39 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Got job 16 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Final stage: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Missing parents: List()\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[135] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/10/27 16:08:39 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 52.8 KiB, free 177.4 MiB)\n",
      "23/10/27 16:08:39 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.6 KiB, free 177.4 MiB)\n",
      "23/10/27 16:08:39 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.16.5.112:41715 (size: 20.6 KiB, free: 177.5 MiB)\n",
      "23/10/27 16:08:39 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[135] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/10/27 16:08:39 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "23/10/27 16:08:39 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 106) (172.16.5.112, executor driver, partition 0, PROCESS_LOCAL, 7951 bytes) \n",
      "23/10/27 16:08:39 INFO Executor: Running task 0.0 in stage 19.0 (TID 106)\n",
      "23/10/27 16:08:39 INFO BlockManager: Found block rdd_13_0 locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------------+------------+--------+----------------+--------+---------+---------+------------+----------------+----------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|          Job Id|   Experience|Qualifications|Salary Range|location|         Country|latitude|longitude|Work Type|Company Size|Job Posting Date|Preference|           Job Title|                Role|  Job Portal|     Job Description|              skills|    Responsibilities|             Company|Average|\n",
      "+----------------+-------------+--------------+------------+--------+----------------+--------+---------+---------+------------+----------------+----------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|1089843540111562|5 to 15 Years|        M.Tech|   $59K-$99K| Douglas|     Isle of Man| 54.2361|  -4.5481|   Intern|       26801|      2022-04-24|    Female|Digital Marketing...|Social Media Manager|    Snagajob|Social Media Mana...|Social media plat...|Manage and grow s...|   Icahn Enterprises|   79.0|\n",
      "| 398454096642776|2 to 12 Years|           BCA|  $56K-$116K|Ashgabat|    Turkmenistan| 38.9697|  59.5563|   Intern|      100340|      2022-12-19|    Female|       Web Developer|Frontend Web Deve...|    Idealist|Frontend Web Deve...|HTML, CSS, JavaSc...|Design and code u...|PNC Financial Ser...|   86.0|\n",
      "| 481640072963533|0 to 12 Years|           PhD|  $61K-$104K|   Macao|Macao SAR, China| 22.1987| 113.5439|Temporary|       84525|      2022-09-14|      Male|  Operations Manager|Quality Control M...|Jobs2Careers|Quality Control M...|Quality control p...|Establish and enf...|United Services A...|   82.5|\n",
      "+----------------+-------------+--------------+------------+--------+----------------+--------+---------+---------+------------+----------------+----------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 16:08:39 INFO Executor: 1 block locks were not released by task 0.0 in stage 19.0 (TID 106)\n",
      "[rdd_13_0]\n",
      "23/10/27 16:08:39 INFO Executor: Finished task 0.0 in stage 19.0 (TID 106). 4959 bytes result sent to driver\n",
      "23/10/27 16:08:39 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 106) in 87 ms on 172.16.5.112 (executor driver) (1/1)\n",
      "23/10/27 16:08:39 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "23/10/27 16:08:39 INFO DAGScheduler: ResultStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.099 s\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/10/27 16:08:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "23/10/27 16:08:39 INFO DAGScheduler: Job 16 finished: showString at NativeMethodAccessorImpl.java:0, took 0.104658 s\n"
     ]
    }
   ],
   "source": [
    "dff=load()\n",
    "dff.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 16:06:15 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Job Id: string, Experience: string, Qualifications: string, Salary Range: string, location: string, Country: string, latitude: string, longitude: string, Work Type: string, Company Size: string, Job Posting Date: string, Preference: string, Contact Person: string, Contact: string, Job Title: string, Role: string, Job Portal: string, Job Description: string, Benefits: string, skills: string, Responsibilities: string, Company: string, Company Profile: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
